{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "远在古希腊时期，发明家就梦想着创造能自主思考的机器。神话人物皮格马利翁、代达罗斯和赫淮斯托斯可以被看作传说中的发明家，而加拉迪亚、塔罗斯和潘多拉则可以被视为人造生命。\n",
    "\n",
    "当人类第一次构思可编程计算机时，就已经在思考计算机能否变得智能（尽管这距造出第一台计算机已有一百多年）。如今，**人工智能**（artificial intelligence，AI）已经成为一个具有众多实际应用和活跃研究课题的领域，并且正在蓬勃发展。我们期望通过智能软件自动地处理常规劳动、理解语音或图像、帮助医学诊断和支持基础科学研究。\n",
    "\n",
    "在人工智能的早期，那些对人类智力来说非常困难、但对计算机来说相对简单的问题得到迅速解决，比如，那些可以通过一系列形式化的数学规则来描述的问题。人工智能的真正挑战在于解决那些对人来说很容易执行、但很难形式化描述的任务，如识别人们所说的话或图像中的脸。对于这些问题，我们人类往往可以凭借直觉轻易地解决。\n",
    "\n",
    "针对这些比较直观的问题，本书讨论一种解决方案。该方案可以让计算机从经验中学习，并根据层次化的概念体系来理解世界，而每个概念则通过与某些相对简单的概念之间的关系来定义。让计算机从经验中获取知识，可以避免由人类来给计算机形式化地指定它需要的所有知识。层次化的概念让计算机构建较简单的概念来学习复杂概念。如果绘制出这些概念如何建立在彼此之上的图，我们将得到一张“深”（层次很多）的图。基于这个原因，我们称这种方法为**AI深度学习**（deep learning）。\n",
    "\n",
    "AI许多早期的成功发生在相对朴素且形式化的环境中，而且不要求计算机具备很多关于世界的知识。例如，IBM的深蓝（Deep Blue）国际象棋系统在1997年击败了世界冠军Garry Kasparov。显然国际象棋是一个非常简单的领域，因为它仅含有64个位置并只能以严格限制的方式移动32个棋子。设计一种成功的国际象棋策略是巨大的成就，但向计算机描述棋子及其允许的走法并不是挑战的困难所在。国际象棋完全可以由一个非常简短的、完全形式化的规则列表来描述，并可以容易地由程序员事先准备好。\n",
    "\n",
    "讽刺的是，抽象和形式化的任务对人类而言是最困难的脑力任务之一，但对计算机而言却属于最容易的。计算机早就能够打败人类最好的象棋选手，但直到最近计算机才在识别对象或语音任务中达到人类平均水平。一个人的日常生活需要关于世界的巨量知识。很多这方面的知识是主观的、直观的，因此很难通过形式化的方式表达清楚。计算机需要获取同样的知识才能表现出智能。人工智能的一个关键挑战就是如何将这些非形式化的知识传达给计算机。\n",
    "\n",
    "一些人工智能项目力求将关于世界的知识用形式化的语言进行硬编码（hardcode）。计算机可以使用逻辑推理规则来自动地理解这些形式化语言中的申明。这就是众所周知的人工智能的**知识库**（knowledge base）方法。然而，这些项目最终都没有取得重大的成功。其中最著名的项目是Cyc。Cyc包括一个推断引擎和一个使用CycL语言描述的声明数据库。这些声明是由人类监督者输入的。这是一个笨拙的过程。人们设法设计出足够复杂的形式化规则来精确地描述世界。例如，Cyc不能理解一个关于名为Fred的人在早上剃须的故事。它的推理引擎检测到故事中的不一致性：它知道人没有电气零件，但由于Fred正拿着一个电动剃须刀，它认为实体“正在剃须的Fred”（“FredWhileShaving”）含有电气部件。因此它产生了这样的疑问——Fred在刮胡子的时候是否仍然是一个人。\n",
    "\n",
    "依靠硬编码的知识体系面对的困难表明，AI系统需要具备自己获取知识的能力，即从原始数据中提取模式的能力。这种能力被称为**机器学习**（machine learning）。引入机器学习使计算机能够解决涉及现实世界知识的问题，并能作出看似主观的决策。比如，一个被称为**逻辑回归**（logistic regression）的简单机器学习算法可以决定是否建议剖腹产。而同样是简单机器学习算法的**朴素贝叶斯**（naive Bayes）则可以区分垃圾电子邮件和合法电子邮件。\n",
    "\n",
    "这些简单的机器学习算法的性能在很大程度上依赖于给定数据的**表示**（representation）。例如，当逻辑回归被用于判断产妇是否适合剖腹产时，AI系统不会直接检查患者。相反，医生需要告诉系统几条相关的信息，诸如是否存在子宫疤痕。表示患者的每条信息被称为一个特征。逻辑回归学习病人的这些特征如何与各种结果相关联。然而，它丝毫不能影响该特征定义的方式。如果将病人的MRI扫描作为逻辑回归的输入，而不是医生正是的报告，它将无法作出有用的预测。MRI扫描的单一像素与分娩过程中并发症之间的相关性微乎其微。\n",
    "\n",
    "在整个计算机科学乃至日常生活中，对表示的依赖都是一个普遍现象。在计算机科学中，如果数据集合被精巧地结构化并被智能地索引，那么诸如搜索之类的操作的处理速度就可以成指数级地加快。人们可以很容易地在阿拉伯数字的表示下进行算术运算，但在罗马数字的表示下运算会比较耗时。因此，毫不奇怪，表示的选择会对机器学习算法的性能产生巨大的影响。图1.1展示了一个简单的可视化例子。\n",
    "\n",
    "![可视化例子](1-attach/1.1-coordinate.png)\n",
    "\n",
    "图1.1：不同表示的例子：假设我们想在散点图中画一条线来分隔两类数据。在左图，我们使用笛卡尔坐标表示数据，这个任务是不可能的。在右图，我们用极坐标表示数据，可以用垂直线简单地解决这个任务。\n",
    "\n",
    "许多人工智能任务都可以通过以下方式解决：先提取一个合适的特征集，然后将这些特征提供给简单的机器学习算法。例如，对于通过声音鉴别说话者的任务来说，一个有用的特征是对其声道大小的估计。这个特征为判断说话者是男性、女性还是儿童提供了有力线索。\n",
    "\n",
    "然而，对于许多任务来说，我们很难知道应该提取哪些特征。例如，假设我们想编写一个程序来检测照片中的车。我们知道，汽车有轮子，所以我们可能会想用车轮的存在与否作为特征。不幸的是，我们难以准确地根据像素值来描述车轮看上去像什么。虽然车轮具有简单的几何形状，但它的图像可能会因场景而异，如落在车轮上的阴影、太阳照亮的车轮的金属零件、汽车的挡泥板或者遮挡的车轮一部分的前景物体等等。\n",
    "\n",
    "解决这个问题的途径之一是使用机器学习来发掘表示本身，而不仅仅把表示映射到输出。这种方法我们称之为**表示学习**（representation learning）。学习到的表示往往比手动设计的表示表现得更好。并且它们只需最少的人工干预，就能让AI系统迅速适应新的任务。表示学习算法只需几分钟就可以为简单的任务发现一个很好的特征集，对于复杂任务则需要几小时到几个月。手动为一个复杂的任务设计特征需要耗费大量的人工时间和精力；甚至需要花费整个社群研究人员几十年的时间。\n",
    "\n",
    "表示学习算法的典型例子是**自编码器**（autoencoder）。自编码器由一个**编码器**（encoder）函数和一个**解码器**（decoder）函数组合而成。编码器函数将输入数据转换为一种不同的表示，而解码器函数则将这个新的表示转换到原来的形式。我们期望当输入数据经过编码器和解码器之后尽可能多地保留信息，同时希望新的表示有各种好的特性，这也是自编码器的目标。为了实现不同的特性，我们可以设计不同形式的自编码器。\n",
    "\n",
    "当设计特征或设计用于学习特征的算法时，我们的目标通常是分离出能解释观察数据的**变差因素**（factors of variation）。在此背景下，“因素”这个词仅指代影响的不同来源；因素通常不是乘性组合。这些因素通常是不能被直接观察到的量。相反，它们可能是现实世界中观察不到的物体或者不可预测的力，但会影响可预测的量。为了对观察到的数据提供有用的简化解释或推断其原因，它们还可能以概念的形式存在于人类的思维中。它们可以被看作数据的概念或者抽象，帮助我们了解这些数据的丰富多样性。当分析语音记录时，变差因素包括说话者的年龄、性别、他们的口音和他们正在说的词语。当分析汽车的图像时，变差因素包括汽车的位置、它的颜色、太阳的角度和亮度。\n",
    "\n",
    "在许多现实的人工智能应用中，困难主要源于多个变差因素同时影响着我们能够观察到的每一个数据。比如，在一张包含红色汽车的图片中，其单个像素在夜间可能会非常接近黑色。汽车轮廓的形状取决于视角。大多数应用需要我们理清变差因素并忽略我们不关心的因素。\n",
    "\n",
    "显然，从原始数据中提取如此高层次、抽象的特征是非常困难的。许多诸如说话口音这样的变差因素，只能通过对数据进行复杂的、接近人类水平的理解来辨识。这几乎与获得原问题的表示一样困难，因此，乍一看，表示学习似乎并不能帮助我们。\n",
    "\n",
    "**深度学习**（deep learning）通过其他较简单的表示来表达复杂表示，解决了表示学习中的核心问题。\n",
    "\n",
    "深度学习让计算机通过较简单概念构建复杂的概念。图1.2展示了深度学习系统如何通过组合较简单的概念（例如转角和轮廓，它们转而由边线定义）来表示图像中人的概念。深度学习模型的典型例子是前馈深度网络或**多层感知机**（multilayer perceptron，MLP）多层感知机仅仅是一个将一组输入值映射到输出值的数学函数。该函数由许多简单的函数复合而成。我们可以认为不同数学函数的每一次应用都为输入提供了新的表示。\n",
    "\n",
    "![深度学习模型示意图](1-attach/1.2-dl-illustrate.png)\n",
    "\n",
    "图1.2：深度学习模型的示意图。计算机难以理解原始感观输入数据的含义，如表示为像素值集合的图像。将一组像素映射到对象标识的函数非常复杂。如果直接处理，学习或评估此映射似乎是不可能的。深度学习将所需的复杂映射分解为一系列嵌套的简单映射（每个由模型的不同层描述）来解决这一难题。输入展示在**可见层**（visible layer），这样命名的原因是因为它包含我们能观察到的变量。然后是一系列从图像中提取越来越多抽象特征的**隐藏层**（hidden layer）。因为它们的值不在数据中给出，所以将这些层称为“隐藏”；模型必须确定哪些概念有利于解释观察数据中的关系。这里的图像是每个隐藏单元表示的特征的可视化。给定像素，第一层可以轻易地通过比较相邻像素的亮度来识别边缘。由了第一隐藏层描述的边缘，第二隐藏层可以容易地搜索可识别为角和扩展轮廓的边集合。给定第二隐藏层中关于角和轮廓的图像描述，第三隐藏层可以找到轮廓和角的特定集合来检测特定对象的整个部分。最后，根据图像描述中包含的对象部分，可以识别图像中存在的对象。\n",
    "\n",
    "学习数据的正确表示的想法是解释深度学习的一个视角。另一个视角是深度促使计算机学习一个多步骤的计算机程序。每一层表示都可以被认为是并行执行另一组指令之后计算机的存储器状态。更深的网络可以按顺序执行更多的指令。顺序指令提供了极大的能力，因为后面的指令可以参考早期指令的结果。从这个角度上看，在某层激活函数里，并非所有信息都蕴涵着解释输入的变差因素。表示还存储着状态信息，用于帮助程序理解输入。这里的状态信息类似于传统计算机程序中的计数器或指针。它与具体的输入内容无关，但有助于模型组织其处理过程。\n",
    "\n",
    "目前主要有两种度量模型深度的方式。第一种方式是基于评估架构所需执行的顺序指令的数目。假设我们将模型表示为给定输入后，计算对应输出的流程图，则可以将这张流程图中的最长路径视为模型的深度。正如两个使用不同语言编写的等价程序将具有不同的长度；相同的函数可以被绘制为具有不同深度的流程图，其深度取决于我们可以用来作为一个步骤的函数。图1.3说明了语言的选择如何给相同的架构两个不同的衡量。\n",
    "\n",
    "![计算图](1-attach/1.3-compute-graph.png)\n",
    "\n",
    "图1.3：将输入映射到输出的计算图表的示意图，其中每个节点执行一个操作。深度是从输入到输出的最长路径的长度，但这取决于可能的计算步骤的定义。这些图中所示的计算是逻辑回归模型的输出，$\\sigma\\left(w^Tx\\right)$，其中$\\sigma$是logistic sigmoid函数。如果我们使用加法、乘法和logistic sigmoid作为我们计算机语言的元素，那么这个模型深度为三。如果我们将逻辑回归视为元素本身，那么这个模型深度为一。\n",
    "\n",
    "另一种是在深度概率模型中使用的方法，它不是将计算图的深度视为模型深度，而是将描述概念彼此如何关联的图的深度视为模型深度。在这种情况下，计算每个概念表示的计算流程图的深度可能比概念本身的图更深。这是因为系统对较简单概念的理解在给出更复杂概念的信息后可以进一步精细化。例如，一个AI系统观察其中一只眼睛在阴影中的脸部图像时，它最初可能只看到一只眼睛。但当检测到脸部的存在后，系统可以推断第二只眼睛也可能是存在的。在这种情况下，概念的图仅包括两层（关于眼睛的层和关于脸的层），但如果我们细化每个概念的估计将需要额外的n次计算，即计算的图将包含2n层。\n",
    "\n",
    "由于并不总是清楚计算图的深度或概率模型图的深度哪一个是最有意义的，并且由于不同的人选择不同的最小元素集来构建相应的图，因此就像计算机程序的长度不存在单一的正确值一样，架构的深度也不存在单一的正确值。另外，也不存在模型多么深才能修饰为“深”的共识。但相比传统机器学习，深度学习研究的模型涉及更多学到功能或学到概念的组合，这点毋庸置疑。\n",
    "\n",
    "总之，这本书的主题——深度学习是通向人工智能的途径之一。具体来说，它是机器学习的一种，一种能够使计算机系统从经验和数据中得到提高的技术。我们坚信机器学习可以构建出在复杂实际环境下运行的AI系统，并且是唯一切实可行的方法。深度学习是一种特定类型的机器学习，具有强大的能力和灵活性，它将大千世界表示为嵌套的层次概念体系（由较简单概念间的联系定义复杂概念、从一般抽象概括到高级抽象表示）。图1.4说明了这些不同的AI学科之间的关系。图1.5展示了每个学科如何工作的高层次原理。\n",
    "\n",
    "![韦恩图](1-attach/1.4-Venn-diagram.png)\n",
    "\n",
    "图1.4：维恩图展示了深度学习是一种表示学习，也是一种机器学习，可以用于许多（但不是全部）AI方法。维恩图的每个部分包括一个AI技术的示例。\n",
    "\n",
    "![流程图](1-attach/1.5-difference.png)\n",
    "\n",
    "如1.5：流程图展示了AI系统的不同部分如何在不同的AI学科中彼此相关。阴影框表示能从数据中学习的组件。\n",
    "\n",
    "## 1.1 本书面向的读者\n",
    "\n",
    "这本书对各类读者都有一定用处，但我们主要是为两类受众对象而写的。其中一类受众对象是学习机器学习的大学生（本科或研究生），包括那些已经开始职业生涯的深度学习和人工智能研究者。另一类受众对象是没有机器学习或统计背景但希望能快速地掌握这方面知识并在他们的产品或平台中使用深度学习的软件工程师。深度学习在许多软件领域都已被证明是有用的，包括计算机视觉、语音和音频处理、自然语言处理、机器人技术、生物信息学和化学、电子游戏、搜索引擎、网络广告和金融。\n",
    "\n",
    "为了最好地服务各类读者，我们将本书组织为三个部分。第一部分介绍基本的数学工具和机器学习的概念。第二部分介绍最成熟的深度学习算法，这些技术基本上已经得到解决。第三部分讨论某些具有展望性的想法，它们被广泛地认为是深度学习未来的研究重点。\n",
    "\n",
    "读者可以随意跳过不感兴趣或与自己背景不相关的部分。熟悉线性代数、概率和基本机器学习概念的读者可以跳过第一部分，例如，当读者只是想实现一个能工作的系统则不需要阅读超出第二部分的内容。为了帮助读者选择章节，图1.6展示了这本书的高层组织结构的流程图。\n",
    "\n",
    "![部分](1-attach/1.6-parts.png)\n",
    "\n",
    "图1.6：本书的高层组织。从一章到另一章的箭头表示前一章是理解后一章的必备内容。\n",
    "\n",
    "我们假设所有读者都具备计算机科学背景。也假设读者熟悉编程，并且对计算的性能问题、复杂性理论、入门微积分和一些图论术语有基本的了解。\n",
    "\n",
    "## 1.2 深度学习的历史趋势\n",
    "\n",
    "通过历史背景了解深度学习是最简单的方式。这里我们仅指出深度学习的几个关键趋势，而不是提供其详细的历史：\n",
    "\n",
    "* 深度学习有着悠久而丰富的历史，但随着许多不同哲学观点的渐渐消逝，与之对应的名称也渐渐尘封。\n",
    "* 随着可用的训练数据量不断增加，深度学习变得更加有用。\n",
    "* 随着时间的推移，针对深度学习的计算机软硬件基础设施都有所改善，深度学习模型的规模也随之增长。\n",
    "* 随着时间的推移，深度学习已经解决日益复杂的应用，并且精度不断提高。\n",
    "\n",
    "### 1.2.1 神经网络的众多名称和命运变迁\n",
    "\n",
    "我们期待这本书的许多读者都听说过深度学习这一激动人心的新技术，并对一本书提及一个新兴领域的“历史”而感到惊讶。事实上，深度学习的历史可以追溯到20世纪40年代。深度学习看似是一个全新的领域，只不过因为在目前流行的前几年它是相对冷门的，同时也因为它被赋予了许多不同的名称（其中大部分已经不再使用），最近才成为众所周知的“深度学习”。这个领域已经更换了很多名称，它反映了不同的研究人员和不同观点的影响。\n",
    "\n",
    "全面地讲述深度学习的历史超出了本书的范围。然而，一些基本的背景对理解深度学习是有用的。一般来说，目前为止深度学习已经经历了三次发展浪潮：20世纪40年代到60年代深度学习的雏形出现在**控制论**（cybernetics）中，20世纪80年代到90年代深度学习表现为**联结主义**（connectionism），直到2006年，才真正以深度学习之名复兴。图1.7给出了定量的展示。\n",
    "\n",
    "![短语](1-attach/1.7-word-or-phrase.png)\n",
    "\n",
    "图1.7：根据Google图书中短语“控制论”、“联结主义”或“神经网络”频率衡量的人工神经网络研究的历史浪潮（图中展示了三次浪潮的前两次，第三次最近才出现）。第一次浪潮开始于20世纪40年代到20世纪60年代的控制论，随着生物学理论的发展和第一个模型的实现（如感知机），能实现单个神经元的训练。第二次浪潮开始于1980-1995年间的联结主义方法，可以使用反向传播训练具有一两个隐藏层的神经网络。当前第三次浪潮，也就是深度学习，大约始于2006年，并且现在在2016年以书的形式出现。另外两次浪潮类似地出现在书中的时间比相应的科学活动晚得多。\n",
    "\n",
    "我们今天知道的一些最早的学习算法，是旨在模拟生物学习的计算模型，即大脑怎样学习或为什么能学习的模型。其结果是深度学习以**人工神经网络**（artificial neural network，ANN）之名而淡去。彼时，深度学习模型被认为是受生物大脑（无论人类大脑或其他动物的大脑）所启发而设计出来的系统。尽管有些机器学习的神经网络有时被用来理解大脑功能，但它们一般都没有被设计成生物功能的真实模型。深度学习的神经观点受两个主要思想启发。一个想法是大脑作为例子证明智能行为是可能的，因此，概念上，建立智能的直接途径是逆向大脑背后的计算原理，并复制其功能。另一种看法是，理解大脑和人类智能背后的原理也非常有趣，因此机器学习模型除了解决工程应用的能力，如果能让人类对这些基本的科学问题有进一步的认识也将会很有用。\n",
    "\n",
    "现代术语“深度学习”超越了目前机器学习模型的神经科学观点。它诉诸于学习多层次组合这一更普遍的原理，这一原理也可以应用于那些并非受神经科学启发的机器学习框架。\n",
    "\n",
    "现代深度学习的最早前身是从神经科学的角度出发的简单线性模型。这些模型被设计为使用一组n个输入$x_1, ..., x_n$并将它们与一个输出$y$相关联。这些模型希望学习一组权重$w_1, ..., w_n$，并计算它们的输出$f(\\mathbf{x}, \\mathbf{w}) = x_1w_1 + ... + x_nw_n$。如图1.7所示，这第一波神经网络研究浪潮被称为控制论。\n",
    "\n",
    "McCulloch-Pitts神经元是脑功能的早期模型。该线性模型通过检验函数$f(\\mathbf{x}, \\mathbf{w})$的正负来识别两种不同类别的输入。显然，模型的权重需要正确设置后才能使模型的输出对应于期望的类别。这些权重可以由操作人员设定。在20世纪50年代，感知机成为第一个能根据每个类别的输入样本来学习权重的模型。约在同一时期，**自适应线性单元**（adaptive linear element，ADALINE）简单地返回函数$f(\\mathbf{x})$本身的值来预测一个实数，并且它还可以学习从数据预测这些数。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
